---
title: Classification and Regression on Random Dot Product Graphs

# to produce blinded version set to 1
blinded: 0

authors: 
- name: 
  affiliation: Department of YYY, University of XXX

keywords:
- latent structure models
- random dot product graph 
- neuroimaging 
- brain connectivity networks


output: rticles::asa_article
# output: rticles::arxiv_article
# output:
#   pdf_document:
#     keep_tex: true
#     citation_package: natbib
#     number_sections: yes

fontsize: 11pt

# geometry: "left=1in,right=1in,top=1in,bottom=1in"

header-includes:
- \usepackage{setspace}
- \usepackage{float}
- \usepackage{mathtools}
- \usepackage{natbib}
- \usepackage[linesnumbered,ruled,vlined]{algorithm2e} 
- \setcitestyle{numbers,square,comma}
- \usepackage{verbatim}
- \usepackage{amsthm}
- \usepackage{comment}
- \usepackage{amsfonts}
- \usepackage{pdfpages}
- \usepackage{xcolor}
- \usepackage{sectsty}

bibliography: bibliography.bib
biblio-style: apalike

abstract: |
  The random dot product graph (RDPG) has become a powerful modeling tool in uncovering latent structures within graphs. In particular, it has been shown that the RDPG describes a wide range of popular random graph models with rigid latent structures. More recently, joint modeling of mutliple random graphs that share common properties or structures across graphs have been introduced, such as the multilayer RDPG, multiple RPDG, and multilayer stochastic block model. In this work, we use these joint random graph models in the context of statistical learning, such as classification and regression, by introducing the multiple latent structure model, in which the graphs share a common latent structure with different parameters that correspond to different response variables. Then we propose various estimation techniques involving manifold learning to estimate these parameters and in turn predict the responses, with theorems guaranteeing convergence of the predictions. Simulations, as well as applications on brain connectivity networks, verify the performance of our methods. 
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      # eval = FALSE,
                      comment = NA, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.pos = 'H', 
                      fig.align = 'center', 
                      fig.height = 3, 
                      fig.width = 5, 
                      fig.dpi = 300)

options(xtable.comment = FALSE, 
        xtable.table.placement = 'H')

import::from(magrittr, `%>%`, `%<>%`)
import::from(foreach, foreach, `%do%`, `%dopar%`)
library(ggplot2)
library(ggraph)
library(mclust)
source('~/dev/pabm-grdpg/functions.R')
source('~/dev/manifold-block-models/functions.R')

doMC::registerDoMC(parallel::detectCores())
```

```{=tex}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\tr}{\mathrm{Tr}}
\newcommand{\blockdiag}{\mathrm{blockdiag}}
\newcommand{\indep}{\stackrel{\mathrm{ind}}{\sim}}
\newcommand{\iid}{\stackrel{\mathrm{iid}}{\sim}}
\newcommand{\Bernoulli}{\mathrm{Bernoulli}}
\newcommand{\Betadist}{\mathrm{Beta}}
\newcommand{\BG}{\mathrm{BernoulliGraph}}
\newcommand{\Uniform}{\mathrm{Uniform}}
\newcommand{\PABM}{\mathrm{PABM}}
\newcommand{\RDPG}{\mathrm{RDPG}}
\newcommand{\GRDPG}{\mathrm{GRDPG}}
\newcommand{\Multinomial}{\mathrm{Multinomial}}
\newcommand{\Categorical}{\mathrm{Categorical}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\as}{\stackrel{\mathrm{a.s.}}{\to}}
\newcommand{\ER}{\text{Erd\"{o}s-R\'{e}nyi}}
\newcommand{\SBM}{\mathrm{SBM}}
\newcommand{\DCBM}{\mathrm{DCBM}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\MBM}{\mathrm{MBM}}
\newcommand{\LSM}{\mathrm{LSM}}
\newcommand{\MLSM}{\mathrm{MLSM}}
\newcommand{\Poisson}{\mathrm{Poisson}}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
```


# Introduction

Graph and network data are now as ubiquitous as traditional feature data in the fields of sociology (e.g., social networks), neuroimaging (e.g., brain connectivity networks), and deep learning (e.g., graph neural networks). 
As a result, new statistical and machine learning methods have recently been developed to analyze network data. 
One approach is to treat the network as a random graph that comes from some probability model. 
In particular, if we sum up the network in an adjacency matrix $A \in \mathbb{R}^{n \times n}$, then one probability model might be to draw each element $A_{ij}$, which represents the existence of an edge or the edge weight from vertex $i$ to $j$, independently from some distribution, perhaps with a unique parameter for the pair $(i,j)$, e.g., $A_{ij} \indep F_{\theta_{ij}}$. 
The classical example of this is the $\ER$ model \citep{Gilbert:1959}, in which every edge is drawn from the same distribution, typically a Bernoulli distribution, using the same parameter, i.e., $A_{ij} \iid \Bernoulli (p)$. 
The inhomogeneous Bernoulli graph extends this by allowing each edge, represented by $A_{ij}$, to have its own parameter, $P_{ij}$, e.g., in the Bernoulli case, $A_{ij} \indep \Bernoulli(P_{ij})$. 
Typically, the parameters are collected into an edge probability matrix (in the case of unweighted graphs) or edge parameter matrix (in the case of weighted graphs), denoted as $P \in \mathbb{R}^{n \times n}$. 
The network analysis problem in this setting is to estimate $P$ given $A$. 

If the parameter matrix $P$ is unconstrained, the inference problem is overparameterized. 
On the other hand, the classical $\ER$ model is often too restrictive to describe real, observed networks. 
Much work has been done to develop models that are constrained enough for robust statistical inference while being generalizable to describe a wide range of networks.
One such family of random graph models is the random dot product graph (RDPG), first proposed by \citet{10.1007/978-3-540-77004-6_11}, which is a type of latent space graph in which each vertex of the graph has a corresponding latent vector in a low-dimensional Euclidean space $\mathbb{R}^d$, and the edge parameter between each pair of vertices is determined by the dot product of the corresponding vectors. 
In this model, the constraint is the low rank of the parameter matrix $P$, assuming that the latent dimension $d$ is less than the number of vertices $n$. 
Further constraints can be imposed on the RDPG in the form of distributional assumptions on the latent vectors or restricting the latent vectors to lie on subspaces or manifolds in the latent space \citep{athreya2020estimation}.

It has been shown \citep{Koo_2022, rubindelanchy2017statistical} that the RDPG (as well as the generalized random dot product graph, or GRDPG, \citep{rubindelanchy2017statistical}) can describe a wide range of popular random graph models, such as the $\ER$ model, the stochastic block model (SBM) \citep{doi:10.1080/0022250X.1971.9989788}, degree corrected block model (DCBM) \citep{Karrer_2011}, and popularity adjusted block model (PABM) \citep{307cbeb9b1be48299388437423d94bf1}. 
In these examples, the (G)RPDG representation of these networks allows for inference and analysis in more familiar Euclidean space rather than on graphs, and the geometry of the configuration of latent vectors correspond to the parameters of these graph models. 
More specifically, when these network models are described as (G)RDPGs, each latent vector lies on a subspace. 
\citet{athreya2020estimation} and \citet{Passino_2020} extended this to the latent structure model (LSM) and latent structure block model (LSBM) to include latent configurations that consist of nonlinear manifolds. 

In this paper, we apply these highly structured GRDPG models to supervised learning problems in which there are multiple graphs, $G_1, ..., G_L$, each with a response $y^{(1)}, ..., y^{(L)}$. 
Each graph, represented by adjacency matrix $A^{(l)}$, is presumed to be drawn from a GRDPG with parameter set $\theta_l$, and in turn, the response $y_l$ is drawn from a univariate distribution with parameter $\theta_l$.
Since the latent vectors of each $A^{(l)}$ are presumed to be distributed on a structured subset of the latent space, e.g., a manifold, we call this the multiple latent structure model (MLSM). 
Much previous work has been done on multilayer networks, such as the multilayer degree corrected block model of \citet{agterberg2022joint}, multiple random eigen graphs of \citet{8889404}, and the multiple common subspace independent edge graphs of \citet{arroyo2020inference}. 
In each of these models, the main inference problem is to identify parameters that are shared across multiple graphs. 
Our contribution is in applying similar methods to identify parameters specific to each graph, which are in turn used as features for supervised learning. 

The following sections are organized as follows: Section 2 introduces the structured and parameterized 

# Methods

## Notation and Scope

Let $G_1, ..., G_L$ be a collection of undirected graphs where each $G_l = (V_l, E_l)$. 
In this setup, each graph can have its own vertex and edge sets. 
Each graph is represented by adjacency matrices $A^{(1)}, ..., A^{(L)}$, where each $A^{(l)} \in \mathbb{R}^{n_l \times n_l}$ is a symmetric and hollow matrix such that $A^{(l)}_{ij}$ represents the edge weight between vertices $i$ and $j$, and $n_l$ represents the size of the vertex set of the $l$^th^ graph. 
Let $P^{(1)}, ..., P^{(L)}$ be the corresponding collection of edge parameter matrices such that each $A^{(l)}_{ij} \indep H(P^{(l)}_{ij})$ for each $1 \leq i < j \leq n_l$ and $H$ is a specified probability distribution such that $E[A^{(l)}_{ij}] = P^{(l)}_{ij}$. 
For ease of notation, we use $A^{(l)} \sim H(P^{(l)})$ for an adjacency matrix $A^{(l)}$ drawn from edge parameter matrix $P^{(l)}$ in this way.
Denote $X^{(1)}, ..., X^{(L)}$ as the sets of latent vectors corresponding to each graph under the GRDPG framework. Each $X^{(l)} \in \mathbb{R}^{n_l \times d}$, i.e., the graphs share the same latent dimension, and $x_i^{(l)} \in \mathbb{R}^d$ is the latent vector corresponding to the $i$^th^ vertex in the $l$^th^ graph.

## Definitions and Models

We begin by defining the RDPG and the LSM. 

\begin{definition}[Random dot product graph (RDPG) \citep{10.1007/978-3-540-77004-6_11}]
\label{def:rdpg}
Let $\mathcal{X}$ be a subset of $\mathbb{R}^d$ for some latent space dimension $d \geq 1$ such that for any $x_1, x_2 \in \mathcal{X}$, $x_1^\top x_2 \in [0, 1]$. 
Let $F_\theta$ be a distribution with support $\mathcal{X}$ and parameters $\theta$, and sample $x_1, ..., x_n \iid F_\theta$. 
A graph $G$ with adjacency matrix $A$ is a random dot product graph with latent vectors $X = \bigl[x_1 \; \cdots \; x_n\bigr]^\top$ drawn from distribution $F_\theta$ if $A \sim H(X X^\top)$. 

We use the notation $A \sim \RDPG(F_\theta)$ to denote a random adjacency matrix $A$ drawn from latent vectors distributed as $F_\theta$. 
\end{definition}

\begin{remark}
\label{remark:nonunique}
The latent vectors of an RDPG are not unique. 
Suppose that $P = X X^\top$ is the edge parameter matrix of an RDPG with latent positions $X$. 
Then any orthogonal transformation $W$ on $X$ results in the same edge parameter matrix. 
More precisely, let $\tilde{X} = X W$. 
Then it is clear that $\tilde{X} \tilde{X}^\top = X W W^\top X^\top = X X^\top = P$ results in the same edge parameter matrix. 
Thus, there are infinitely many latent vector configurations that can result in the same $P$, but for any two latent vector configurations, there exists an orthogonal mapping that connects the two. 
Similarly, if $A \sim \RDPG(F_{\mu, \Sigma})$ and $F_{\mu, \Sigma}$ is the normal distribution with mean vector $\mu$ and covariance matrix $\Sigma$, then $A \sim \RDPG(F_{W \mu, W \Sigma W^\top})$ is an equivalent RDPG for any orthogonal matrix $W$. 
\end{remark}

\begin{remark}
\label{remark:grdpg}
The RDPG requires that $P = X X^\top$ be a positive semidefinite matrix. 
To include matrices that are not positive semidefinite, the generalized random dot product graph (GRDPG) was introduced by \citet{rubindelanchy2017statistical}, in which $P = X I_{p,q} X^\top$. 
Here, $I_{p,q}$ is a diagonal matrix of $p$ $1$'s followed by $q$ $-1$'s. 
While the focus of this work is on the RDPG, the theoretical results also apply to the GRDPG. 
\end{remark}

\begin{definition}[Latent structure model (LSM) \citep{athreya2020estimation}]
\label{def:lsm}
Let $\mathcal{C} \subset \mathcal{X} \subset \mathbb{R}^d$ be a smooth, nonintersecting one-dimensional manifold on the domain of a RDPG as defined in definition \ref{def:rdpg}, parameterized by function $p(t) : [0, 1] \to \mathcal{C}$. 
Then if $t_1, ..., t_n \iid F_\theta$ for some distribution $F_\theta$ with support $[0, 1]$ and parameter $\theta$, each $x_i = p(t_i)$, and $ A \sim H(X X^\top)$ for $X = \bigl[x_1 \; \cdots \; x_n\bigr]^\top$, $A$ is the adjacency matrix of a latent structure model on curve $\mathcal{C}$ with parameterization $p$ and underlying distribution $F_\theta$. 

We use the notation $A \sim \LSM(\mathcal{C}, F_\theta)$ or $A \sim \LSM(p, F_\theta)$ to denote an adjacency matrix $A$ drawn as an LSM on curve $\mathcal{C}$ or its parameterization $p$ with underlying distribution $F_\theta$. 
\end{definition}

\begin{remark}
\label{rem:lsm-mixture}
Although \citet{athreya2020estimation} defined the LSM by a single one-dimensional manifold $\mathcal{C}$ in the latent space, in this paper, we will allow for the existence of multiple one-dimensional manifolds, $\mathcal{C}_1, ..., \mathcal{C}_K$, (i.e., mixture of manifolds distribution). 
This type of latent space mixture distribution is observed in networks with community structure \citep{10.5555/3122009.3242083}. 
For estimation, if the membership of each latent vector to the manifolds is known, then each manifold can be learned separately using the vectors that belong to that manifold. 
If the memberships are not known, then we use an iterative algorithm to both cluster the latent vectors to a known number of manifolds and learn the manifolds using the cluster assignments. 

In the case of a mixture of $K$ curves, we use the notation $A \sim \LSM(\{\mathcal{C}_k\}_K, F_\theta, \alpha)$ or $A \sim \LSM(\{p_k\}_K, F_\theta, \alpha)$, where $\alpha = (\alpha_1, ..., \alpha_K)$, $\sum_{k=1}^K \alpha_k = 1$ is the mixture parameter. 
For simplicity, we only consider the case where the underlying distribution is the same for each curve. 
\end{remark}

A plausible inference task in the RDPG is to estimate the original latent vectors. 
The adjacency spectral embedding \citep{doi:10.1080/01621459.2012.699795} is a consistent estimator of the latent vectors, up to some unknown orthogonal transformation.

\begin{remark}[Sparsity parameter]
In many real networks, the degree of each vertex often does not grow proportionally with the size of the network. 
To account for this, a sparsity factor $\rho_n \in (0, 1]$ is introduced in the edge probabilities, i.e., $P_{ij} \leftarrow \rho_n P_{ij}$, for some sequence $\{\rho_n\}$. 
Oftentimes the additional constraint of $\lim\limits_{n \to \infty} \rho_n = 0$ is included. 
For example, a sparse SBM has edge probabilities $P_{ij} = \rho_n \theta_{z_i, z_j}$, for which we use the notation $A \sim \SBM(z, \{\theta_{k \ell}\}_K; \rho_n)$ or $A \sim \SBM(\alpha, \{\theta_{k \ell}\}_K; \rho_n)$, depending on whether we treat the labels as random or fixed. 
Then the expected degree grows as $O(n \rho_n)$ instead of linearly as $O(n)$. 
For the sake of unifying the sparse and dense regimes, we also allow for the special case $\rho_n = 1$ and include the sparsity factor throughout, unless otherwise stated. 
Finally, we also note that while $\rho_n$ limits the rate of growth of the expected degree, our theoretical results still require $n \rho_n$ to diverge to infinity, albeit at a slower rate than $O(n)$. 
For example, if $\rho_n \propto 1 / \sqrt{n}$, then the expected degree for each vertex of the SBM is $O(\sqrt{n})$. 
In general, for consistency in estimation and inference, most results on Bernoulli random graphs require $n \rho_n = \omega \big( (\log n)^c \big)$ for some $c > 1$. 
(See \citet{JMLR:v18:16-480}, \citet{https://doi.org/10.48550/arxiv.2106.09840}, and \citet{rubindelanchy2017statistical} for further discussion.)
\end{remark}

\begin{definition}[Adjacency spectral embedding (ASE) \citep{doi:10.1080/01621459.2012.699795}]
\label{def:ase}
Let $A = V \Lambda V^\top$ be the spectral decomposition of $A$. 
Define $\lambda_i$ as the $i^{th}$ largest eigenvalue of $A$ and $v_i$ by its corresponding eigenvector, and let $V_d = \bigl[ v_1 \; \cdots \; v_d \bigr]$ and $\Lambda_d = \diag(\lambda_1, ..., \lambda_d)$. 
Then $\hat{X} = V_d |\Lambda|^{1/2}$ is the $d$-dimensional adjacency spectral embedding of $A$. 
\end{definition}

\begin{theorem}[Consistency of the ASE \citep{lyzinski2014}]
\label{theorem:ase-consistency}
Suppose the sparsity parameter is such that $n \rho_n = \omega (\log^{4 c} n)$ for some constant $c > 1$. 
Then for some orthogonal transformation $W \in \mathbb{O}(d)$, 
$$\max_i \|W \hat{x}_i - x_i \| = O_P \bigg( \frac{\log^c n}{n^{1/2}} \bigg),$$
where $x_i^\top$ and $\hat{x}_i^\top$ are the rows of $X$ and $\hat{X}$, respectively. 
\end{theorem}

Theorem \ref{theorem:ase-consistency} implies that the embedding vectors of the ASE converge to the original latent vectors, up to some unidentifiable orthogonal transformation, and the maximum deviation from the original latent vectors after the orthogonal transformation is bounded by a value that decays to $0$. 
This implies that in the LSM, the ASE can lead to consistent estimation of $F_\theta$, the underlying distribution, and in fact, \citet{athreya2020estimation} showed exactly this. 

\begin{definition}[Multiple latent structure model (MLSM)]
\label{def:mlsm}
Let $\mathcal{C}^{(1)}, ..., \mathcal{C}^{(L)} \subset \mathbb{R}^d$ be a sequence of curves defining the latent positions of a sequence of $L$ LSMs as in definition \ref{def:lsm}. 
Each $\mathcal{C}^{(\ell)}$ is parameterized by function $p^{(\ell)}(t) : [0, 1] \to \mathcal{C}^{(\ell)}$. 
Let $p^{(1)}, ..., p^{(L)}$ be a sequence of functions with domain $[0, 1]$ that parameterize the curves $\mathcal{C}^{(1)}, ..., \mathcal{C}^{(L)}$, let $F$ be a parametric distribution with support $[0, 1]$, and let $\theta_1, ..., \theta_L$ be a sequence of parameters for distribution $F$. 
For each $p^{(\ell)}$, sample $t_1^{(\ell)}, ..., t_{n_\ell}^{(\ell)} \iid F_{\theta^{(\ell)}}$ for some distribution $F$ parameterized by $\theta_\ell$, and let $x_i^{(\ell)} = p_\ell(t_i^{(\ell)})$ for each $i = 1, ..., n_\ell$, again as in definition \ref{def:lsm}. 
Sample a sequence of $L$ adjacency matrices, each as $A^{(\ell)} \indep \LSM(p_\ell, F_{\theta_\ell})$. 
Then the sequence $\{A^{(\ell)}\}_L$ is are the adjacency matrices of a multiple latent structure model with curves $\{C^{(\ell)}\}_L$ parameterized by $\{p^{(\ell)}\}$ and underlying distribution $F$ with parameters $\{\theta_\ell\}_L$. 

We use the notation $A^{(1)}, ..., A^{(L)} \sim \MLSM(\{p^{(\ell)}\}, F, \{\theta_\ell\}_L)$ to denote a sequence of adjacency matrices drawn from an MLSM with parameterizations $\{p^{(\ell)}\}$ and parameters $\{\theta_\ell\}$ on underlying distrubtion $F$. 
\end{definition}

\begin{remark}
\label{remark:mlsm-mixture}
Again as in the case of a single LSM, we also allow for each $A^{(\ell)}$ to be sampled from a latent structure composed of $K$ curves with mixture parameter $\alpha^{(\ell)}$. 
In this case, we use the notation $A^{(1)}, ..., A^{(L)} \sim \MLSM(\{\mathcal{C}_k^{(\ell)}\}_{K, L}, F, \{\theta_\ell\}_L, \{\alpha^{(\ell)}\}_L)$ or $A^{(1)}, ..., A^{(L)} \sim \MLSM(\{p_k^{(\ell)}\}_{K, L}, F, \{\theta_\ell\}_L, \{\alpha^{(\ell)}\}_L)$.
\end{remark}

### Connections to Related Models

In the MLSM defined in definition \ref{def:mlsm}, the only commonality that we assume from graph to graph is that they are all LSMs with the same underlying distribution family. 
If we impose further restrictions, namely that the latent structures are linear, we obtain the multilayer random dot product graph \citep{jones2021multilayer}.

\begin{example}[Comparison to the multilayer DCBM \citep{agterberg2022joint}, multi-RDPG \citep{nielsen2018multiple}, and MREG \citep{8889404}]
In the $K$-community DCBM, the probability of an edge between a pair of vertices is given by

$$A_{ij} \indep \Bernoulli(\omega_i \omega_j B_{z_i, z_j}),$$

where $z_i \in \{1, ..., K\}$ is the community label for vertex $i$, $B_{k, \ell}$ is the block connectivity between communities $k$ and $\ell$, and $\omega_i$ is the degree correction parameter for vertex $i$. 
In order to preserve identifiability and uniqueness, a common constraint on these parameters is to set $\sum_{i : z_i = k} \omega_i^2 = 1$ \citep{Karrer_2011}. 
As in 

The edge parameter matrix of a $K$-community DCBM with $n$ vertices can be decomposed as follows:

$$P = \Omega B \Omega^\top,$$

where $B$ is a $K \times K$ matrix of block connectivities and $\Omega$ is an $n \times K$ matrix such that $\Omega_{ik} = \omega_i$ if vertex $i$ is in community $k$ and $0$ otherwise. 
Then it is clear that if $B$ is positive semidefinite matrix of rank $K$, $P$ can also be viewed as a $K$-dimensional RDPG, since the rows of $\Omega$ are normalized and can be seen as eigenvectors, and $B$ is full rank and can be decomposed into a diagonal matrix and a rotation matrix, i.e., $P = (\Omega V) \Lambda (\Omega V)^\top$. 
Furthermore, this matrix decomposition implies that the latent vectors lie on one of $K$ line segments that intersect at the origin \citep{rubindelanchy2017statistical}. 
Thus, the DCBM is a special case of the LSM in which there are $K$ latent ``linear curves'' (i.e., lines) in $\mathbb{R}^K$. 

The multilayer DCBM as described by \citet{agterberg2022joint} extends this to a sequence of DCBMs with the same community structure, allowing the block connectivities and degree correction parameters to change for each layer but keeping the same community structure throughout, i.e., each element of $P^{(\ell)} = \Omega^{(\ell)} B^{(\ell)} (\Omega^{(\ell)})^\top$ can vary with $\ell$ but $\Omega^{(\ell)} = 0 \iff \Omega^{(\ell')} = 0$ and similarly, $\Omega^{(\ell)} > 0 \iff \Omega^{(\ell')} > 0$, for every pair $(\ell, \ell')$. 
On the other hand, if the further restriction of $\Omega^{(\ell)} = \Omega^{(\ell')}$ for all $(\ell, \ell')$, i.e., $P^{(\ell)} = \Omega B^{(\ell)} \Omega^\top$, we obtain a special case of the multiple RDPG (multi-RDPG) with the identity link function, as described by \citet{nielsen2018multiple}, or equivalently, a special case of the multiple random eigen graphs model (MREG), as described by \citet{8889404}. 
However, if the community labels are not identical from graph to graph, the sequence of DCBMs cannot be described as a multilayer DCBM, multi-RDPG, or MREG, but it can still be described as an MLSM. 
\end{example}

\begin{example}[Nonlinear MLSM and comparison to MREG and multi-RDPG]
\end{example}

### Classification and Regression on the MLSM

To use the MLSM for classification or regression problems, we assign response variables $y_1, ..., y_L$ to each graph. 
Then if each response $y_\ell$ depends on $p^{(\ell)}(t)$ (the parameterization of the $\ell^{th}$ curve) or $\theta_\ell$ (the parameter of the $\ell^{th}$ distribution), or some combination of the two, there is a plausible setup for a predictive modeling task for predicting $y_\ell$ after observing $A^{(\ell)}$. 
Four such scenarios are given:

\begin{definition}[MLSM for classification 1]
\label{def:mlsm-c-1}
Let $A^{(1)}, ..., A^{(L)} \sim \MLSM(\{p^{(\ell)}\}, F, \{\theta_\ell\}_L)$, $y_1, ..., y_L \in \{1, ..., M\}$ be labels for each of the $L$ graphs of the MLSM, and each $\theta_\ell$ take on one of $M$ values $\{\phi_1, ..., \phi_M\}$ corresponding to its response, $y_\ell$, i.e., $\theta_\ell = \phi_{y_\ell}$. 

We observe the adjacency matrices $A^{(1)}, ..., A^{(L)}$ and the first $r$ labels $y_1, ..., y_r$, and the task is to predict the unknown labels $y_{r+1}, ..., y_L$. 

This can be described by the following generative model:

\begin{enumerate}
  \item Draw labels $y_1, ..., y_L \iid \Categorical(\{1, ..., M\}, \{\pi_1, ..., \pi_M\})$.
  \item Set each $\theta_\ell = \phi_{y_\ell}$.
  \item Draw adjacency matrices as $A^{(1)}, ..., A^{(L)} \sim \MLSM(\{p^{(\ell)}\}, F, \{\theta_\ell\}_L)$.
\end{enumerate}
\end{definition}

\begin{definition}[MLSM for classification 2]
\label{def:mlsm-c-2}
Let $A^{(1)}, ..., A^{(L)} \sim \MLSM(\{\mathcal{C}^{(\ell)}\}, F, \{\theta_\ell\}_L)$, $y_1, ..., y_L \in \{1, ..., M\}$ be labels for each of the $L$ graphs of the MLSM, and each $\mathcal{C}^{(\ell)}$ take on one of $M$ functional forms $\{p^{(1)}(t), ..., p^{(M)}(t)\}$ corresponding to its response variable, $y_\ell$, i.e., $\mathcal{C}^{(\ell)}$ is parameterized by $p^{(y_\ell)}(t)$. 

We observe the adjacency matrices $A^{(1)}, ..., A^{(L)}$ and the first $r$ labels $y_1, ..., y_r$, and the task is to predict the unknown labels $y_{r+1}, ..., y_L$. 

This can be described by the following generative model:

\begin{enumerate}
  \item Draw labels $y_1, ..., y_L \iid \Categorical(\{1, ..., M\}, \{\pi_1, ..., \pi_M\})$.
  \item Set each $p^{(\ell)}(t) = p^{(y_\ell)}(t)$.
  \item Draw adjacency matrices as $A^{(1)}, ..., A^{(L)} \sim \MLSM(\{p^{(\ell)}\}, F, \{\theta_\ell\}_L)$.
\end{enumerate}

As in remark \ref{remark:mlsm-mixture}, we also allow for each adjacency matrix $A^{(\ell)}$ to be drawn from a latent structure consisting of multiple curves $\{p^{(\ell)}_k\}_K$. 
\end{definition}

\begin{definition}[MLSM for regression 1]
\label{def:mlsm-r-1}
Let $A^{(1)}, ..., A^{(L)} \sim \MLSM(\{p^{(\ell)}\}, F, \{\theta_\ell\}_L)$, and suppose that for each $\ell = 1, ..., L$, the $\ell^{th}$ graph is coupled with a response variable, $y_\ell$, as $y_\ell \indep \mathcal{N}(\theta_\ell^\top \beta, \sigma^2)$. 

We observe the adjacency matrices $A^{(1)}, ..., A^{(L)}$ and the first $r$ response variables $y_1, ..., y_r$. 
In this setting, there are two plausible inference tasks. 
The first is to estimate the coefficient vector $\beta$. 
The second is to predict the unobserved response variables $y_{r+1}, ..., y_L$. 
\end{definition}

\begin{definition}[MLSM for regression 2]
\label{def:mlsm-r-2}
Let $A^{(1)}, ..., A^{(L)} \sim \MLSM(\{p^{(\ell)}\}, F, \{\theta_\ell\}_L)$ and each $p^{(\ell)}(t) = p(t; \gamma_\ell)$, where $\gamma_\ell$ is the vector of parameters for function $p$. 
Suppose that for each $\ell = 1, ..., L$, the $\ell^{th}$ graph is coupled with a response variable, $y_\ell$, as $y_\ell \indep \mathcal{N}(\gamma_\ell^\top \beta, \sigma^2)$. 

We observe the adjacency matrices $A^{(1)}, ..., A^{(L)}$ and the first $r$ response variables $y_1, ..., y_r$. 
In this setting, there are two plausible inference tasks. 
The first is to estimate the coefficient vector $\beta$. 
The second is to predict the unobserved response variables $y_{r+1}, ..., y_L$. 

As in remark \ref{remark:mlsm-mixture}, we also allow for each adjacency matrix $A^{(\ell)}$ to be drawn from a latent structure consisting of multiple curves $\{p^{(\ell)}_k\}_K$. 
\end{definition}

In all of these settings, the ASE of $A^{(\ell)}$ provides some insight into both $p^{(\ell)}$ and $\theta_\ell$. 
\citet{athreya2020estimation} showed that with some additional information, the ASE of $A^{(\ell)}$ can lead to a consistent estimator for $\theta_\ell$. 

## Main Results

We propose our methodology for classification and regression in three steps:

1. Curve-fitting algorithm to recover the LSM curves from the ASEs.
2. Quasi-maximum likelihood estimation to recover the parameters of the underlying distributions. 
3. Classification or regression to determine the relationship between the parameters and the responses. 

### Estimation of the LSM Curves

For curve-fitting, we will restrict the LSM curves to non-self-intersecting Bezier polynomials \citep{10.5555/320367}. 
A polynomial curve of degree $R$ in $\mathbb{R}^d$ is uniquely defined by a Bezier polynomial within the space of Bezier polynomials of degree at most $R$, up to reverse order, although there are infinitely many Bezier polynomials of degree greater than $R$ that can describe the curve \citep{SANCHEZREYES2022102118}. 
We further restrict the curves to the case $p(0) = 0$, i.e., the curve begins at the origin. 
This is consistent with well-known models such as the DCBM and PABM when viewed as LSMs or mixtures of LSMs \citep{Koo_2022, rubindelanchy2017statistical}, as well as real networks that can adequately be described as LSMs \citep{athreya2020estimation}. 
This constraint also removes the reverse order nonidentifiability. 
Thus, if $R$, the degree of the polynomial, can be determined, and the curve begins at the origin, a collection of $n \geq R$ unique vectors on the curve and not at the origin can determine the unique Bezier polynomial that describes the curve. 

A Bezier polynomial of this form, is defined as 
\begin{equation}
\label{eq:bezier}
p(t; b_1, ..., b_R) = \sum_{r=1}^R \binom{R}{r} b_r (1-t)^r t^{R-r},
\end{equation}
where $b_r \in \mathbb{R}^d$ are Bezier coefficients (control points). 
In the scenario in which an adjacency matrix $A$ sampled from an LSM with curve $p$ that can be described by a Bezier polynomial, the ASE of $A$ consists of embedding vectors that lie on or near a rotation of the curve. 
Then if embedding vectors $x_1, ..., x_n \in \mathbb{R}^d$ are such that $x_i = p(t_i) + \epsilon_i$ (for simplicity, redefine $p$ to be the rotated version of the original curve), the squared loss function for the curve fit is of the form
\begin{equation}
\label{eq:bezier-loss-full}
L(t_1, ..., t_n; b_1, ..., b_R) = \sum_{i=1}^n \big( x_i - p(t_i; b_1, ..., b_R) \big)^2.
\end{equation}

Letting $X = \bigl[ x_1 \; \cdots \; x_n \bigr]^\top \in \mathbb{R}^{n \times p}$, $T \in \mathbb{R}^{n \times R}$ such that $T_{ir} = \binom{R}{r} (1-t_i)^{R-r} t_i^r$, $b = \bigl[b_1 \; \cdots \; b_R\bigr]^\top \in \mathbb{R}^{R \times d}$, the loss function can be rewritten as
\begin{equation}
\label{eq:bezier-loss}
L(T, b) = \|X - T b\|_F^2,
\end{equation}
where $\|\cdot\|_F$ is the Frobenius norm. Then if $t_1, ..., t_n$ are known, the solution to the least squares best fit Bezier coefficients is given by the ordinary least squares estimate, 
\begin{equation}
\label{eq:bezier-ols}
\hat{b} = (T^\top T)^{-1} T^\top X.
\end{equation}

On the other hand, if the coefficients are known but the timepoints $t_1, ..., t_n$ are not, this turns into $n$ individual minimization problems for polynomials of degree $2 R$. 
More precisely, $L(t_1, ..., t_n; b_1, ..., b_R) = \sum_{i=1}^n L(t_i; b_1, ..., b_R)$ where 
\begin{equation}
\label{eq:bezier-ls}
L(t_i; b_1, ..., b_R) = x_i - \sum_r \binom{R}{r} b_r (1-t_i)^{R-r} t_i^r. 
\end{equation}
To solve, we find the at most $2 R - 1$ roots of its derivative, 
\begin{equation}
\label{eq:dbezier}
\dot{L}(t_i; b_1, ..., b_R) = \bigg(\sum_{r=1}^R \binom{R}{r} (-1)^r c_r t_i^r \bigg) \bigg(\sum_{r=0}^{R-1} \binom{R-1}{r} (-1)^r c_{r+1} t_i^r \bigg) = 0,
\end{equation}
where $c_r = \sum_{s=1}^r (-1)^{r-s} \binom{r}{s} b_s$. 
Since each $t_i$ can be solved separately, this method is highly parallelizable. 

For initialization, the ordering of $t_1, ..., t_n$ is determined via a one-dimensional Isomap embedding \citep{Tenenbaum2000-ff}. 

Combining equation \ref{eq:bezier-ols} with the solutions to equation \ref{eq:dbezier} for each $i = 1, ..., n$ provides an alternating coordinate descent algorithm, as outlined in algorithm \ref{alg:bezier-fit}.

\begin{algorithm}[h]
\label{alg:bezier-fit}
\DontPrintSemicolon
\SetAlgoLined
\setstretch{1.1}
\KwData{Adjacency matrix $A$, embedding dimension $d$, polynomial degree $R$, Isomap neighborhood parameter $\lambda$, stopping criterion $\epsilon$.}
\KwResult{Bezier coefficients $\hat{b}$, timepoint values $\hat{t}_1, ..., \hat{t}_n$.}
Compute $\hat{X} \in \mathbb{R}^{n \times d}$, the ASE of $A$.\;
Initialize timepoints $\hat{t}_1, ..., \hat{t}_n$, first via a one-dimensional Isomap embedding of $\hat{X}$ using neighborhood parameter $\lambda$, then by normalizing the embedding to $[0, 1]$.\;
\Repeat {the change in equation \ref{eq:bezier-ls} is less than $\epsilon$.} {
  Fit $\hat{b}$ by equation \ref{eq:bezier-ols}.\;
  \For {$i = 1, ..., n$} {
    Fit $\hat{t}_i$ by equation by finding the roots of equation \ref{eq:dbezier} and choosing the roots that minimizes equation \ref{eq:bezier-ls}.\;
  }
}
\caption{Procedure for estimating an LSM curve as a Bezier polynomial from an adjacency matrix.}
\end{algorithm}

In the case of a mixture of LSMs (as in remark \ref{rem:lsm-mixture}), algorithm \ref{alg:bezier-fit} is modified by fitting Bezier polynomials for each vertex label (if the labels are known) or by estimating the labels via an alternating coordinate descent algorithm outlined in algorithm \ref{alg:bezier-fit-mixture}.

\begin{algorithm}[H]
\label{alg:bezier-fit-mixture}
\DontPrintSemicolon
\SetAlgoLined
\setstretch{1.1}
\KwData{Adjacency matrix $A$, embedding dimension $d$, polynomial degree $R$, number of latent structures $K$, Isomap neighborhood parameter $\lambda$, stopping criterion $\epsilon$.}
\KwResult{Bezier coefficients $\hat{b}$, timepoint values $\hat{t}_1, ..., \hat{t}_n$, curve memberships $\hat{z}_1, ..., \hat{z}_n$.}
Compute $\hat{X} \in \mathbb{R}^{n \times d}$, the ASE of $A$.\;
Initialize $\hat{z}_1, ..., \hat{z}_n \in \{1, ..., K\}$, the membership of each vertex to each latent curve.\;
\For {$k = 1, ..., K$} {
  Subset the rows of $\hat{X}$ by $z_i = k$ to obtain $\hat{x}_{k_1}, ..., \hat{x}_{k_{n_k}}$, where $n_k$ is the number of vertices with label $k$.\;
  Initialize timepoints $\hat{t}_{k_1}, ..., \hat{t}_{k_{n_k}}$, first via a one-dimensional Isomap embedding of $\hat{x}_{k_1}, ..., \hat{x}_{k_{n_k}}$ using neighborhood parameter $\lambda$, then by normalizing the embedding to $[0, 1]$.\;
}
\Repeat {the change in equation \ref{eq:bezier-ls} is less than $\epsilon$.} {
  \For {$k = 1, ..., K$} {
    Fit $\hat{b}^{(k)}$ by equation \ref{eq:bezier-ols}, using the row subsets of $T$ and $X$ such that $z_i = k$.\;
    \For {$i = 1, ..., n_k$} {
      Fit $\hat{t}_{k_i}$ by equation by finding the roots of equation \ref{eq:dbezier} and choosing the roots that minimizes equation \ref{eq:bezier-ls}.\;
    }
  }
  \For {$k = 1, ..., n$} {
    Reassign $z_i$ based on the latent curve closest to $\hat{x}_i$. 
  }
}
\caption{Procedure for estimating multiple LSM curves as a Bezier polynomial from an adjacency matrix.}
\end{algorithm}

\begin{lemma}
\label{lem:bezier-consistency}
Suppose $A \sim \LSM(p, F_\theta)$ such that $p : [0, 1] \to \mathbb{R}^d$ is a Bezier polynomial of degree $R$ with coefficients $b$, and $F_\theta$ is a probability distribution with probability density function $f_\theta(t)$ that is absolutely continuous on support $[0, 1]$ and $\min_t f_\theta(t) > 0$ $\forall t \in [0, 1]$. 
Let $\hat{T}$ and $\hat{b}$ be the minimizers of the loss function in equation \ref{eq:bezier-ls} from $A$, and let $\tilde{X} = \hat{T} \hat{b}$, the estimated vectors along the fitted Bezier curve. 
Then as $n \to \infty$, $\|X - \tilde{X} W\|_{2,\infty} \stackrel{p}{\to} 0$ for some $W \in \mathbb{O}(d)$.
\end{lemma}

\begin{proof}
Let $X$ be the true latent positions of the LSM and $\hat{X}$ be the ASE of $A$ drawn from $X$. 
Then $X = T b$, and $\hat{X} = \hat{T} \hat{b} + \hat{E}$, where $\hat{E}$ is the matrix of distances from the estimated positions along the fitted Bezier curves and the embedding vectors. 
Then
$$
\begin{aligned}
\|X - \tilde{X} W\| & = \|T b - \hat{T} \hat{b} W\|_{2,\infty} \\
& = \|T b - (\hat{T} \hat{b} + \hat{E} - \hat{E}) W \|_{2,\infty} \\
& = \|T b - (\hat{T} \hat{b} + \hat{E}) W + \hat{E} W\|_{2,\infty} \\
& \leq \|X - \hat{X} W\|_{2,\infty} + \|\hat{E} W\|_{2,\infty}.
\end{aligned}
$$
The first term $\|X - \hat{X} W\|_{2,\infty}$ converges to $0$ in probability \citep{lyzinski2014}. 
Since the ASE is consistent, the embedding vectors will converge in probability to a Bezier curve, so $\|\hat{E} W\|_{2,\infty} \stackrel{p}{\to} 0$. 
\end{proof}

\begin{lemma}
Suppose $A \sim \LSM(p, F_\theta)$ such that $p : [0, 1] \to \mathbb{R}^d$ is a Bezier polynomial of degree $R$ with coefficients $b$, and $F_\theta$ is a probability distribution with probability density function $f_\theta(t)$ that is absolutely continuous on support $[0, 1]$ and $\min_t f_\theta(t) > 0$ $\forall t \in [0, 1]$. 
Let $\hat{T}$ and $\hat{b}$ be the minimizers of the loss function in equation \ref{eq:bezier-ls} from $A$. 
Then $\|T - \hat{T}\|_{2, \infty} \stackrel{p}{\to} 0$ as if and only if $\|b - \hat{b} W\|_F \stackrel{p}{\to} 0$. 
\end{lemma}

\begin{theorem}[Consistency of algorithm \ref{alg:bezier-fit}]
\label{thm:bezier-consistency}
Suppose $A \sim \LSM(p, F_\theta)$ such that $p : [0, 1] \to \mathbb{R}^d$ is a Bezier polynomial of degree $R$ with finite coefficients $b$, and $F_\theta$ is a probability distribution with probability density function $f_\theta(t)$ that is absolutely continuous on support $[0, 1]$ and $\min_t f_\theta(t) > 0$ $\forall t \in [0, 1]$. 
Let $\hat{T}$ and $\hat{b}$ be the minimizers of the loss function in equation \ref{eq:bezier-ls} from $A$. 
Then as $n \to \infty$, $\|T - \hat{T}\|_{2, \infty} \stackrel{p}{\to} 0$ where $T$ is defined as in equation \ref{eq:bezier-loss}, and $\|b - \hat{b} W \|_F \stackrel{p}{\to} 0$ for some $W \in \mathbb{O}(d)$.
\end{theorem}

\begin{proof}

\end{proof}

### Recovery of the Underlying Distributions

\begin{algorithm}[H]
\label{alg:bezier-param-fit}
\DontPrintSemicolon
\SetAlgoLined
\setstretch{1.1}
\KwData{Adjacency matrix $A$, embedding dimension $d$, probability distribution family $F$, polynomial degree $R$, Isomap neighborhood parameter $\lambda$, stopping criterion $\epsilon$.}
\KwResult{Bezier coefficients $\hat{b}$, timepoint values $\hat{t}_1, ..., \hat{t}_n$.}
Compute $\hat{X} \in \mathbb{R}^{n \times d}$, the $d$-dimensional ASE of $A$.\;
Initialize timepoints $\hat{t}_1, ..., \hat{t}_n$, first via a one-dimensional Isomap embedding of $\hat{X}$ using neighborhood parameter $\lambda$, then by normalizing the embedding to $[0, 1]$.\;
\Repeat {the change in equation \ref{eq:bezier-ls} is less than $\epsilon$.} {
  Fit $\hat{b}$ by equation \ref{eq:bezier-ols}.\;
  \For {$i = 1, ..., n$} {
    Fit $\hat{t}_i$ by equation by finding the roots of equation \ref{eq:dbezier} and choosing the roots that minimizes equation \ref{eq:bezier-ls}.\;
  }
}
Estimate $\hat{\theta}$ from $\hat{t}_1, ..., \hat{t}_n$ via maximum likelihood estimation.\; 
\caption{Procedure for estimating the underlying distribution of a Bezier LSM curve from an adjacency matrix.}
\end{algorithm}

\begin{theorem}[Consistency of algorithm \ref{alg:bezier-param-fit}]
Suppose $A \sim \LSM(p, F_\theta)$ such that $p : [0, 1] \to \mathbb{R}^d$ is a Bezier polynomial of degree $R$ with coefficients $b$, and $F_\theta$ is a probability distribution with probability density function $f_\theta(t)$ that is absolutely continuous on support $[0, 1]$ and $\min_t f_\theta(t) > 0$ $\forall t \in [0, 1]$. 
Let $\hat{t}_1, ..., \hat{t}_n$ and $\hat{b}$ be the minimizers of the loss function in equation \ref{eq:bezier-ls} from $A$, and let $\tilde{theta}$ be the maximum likelihood estimator for $\theta$ using $\hat{t}_1, ..., \hat{t}_n$. 
Then $\tilde{\theta} \stackrel{p}{\to} \theta$. 
\end{theorem}

\begin{proof}
By theorem \ref{thm:bezier-consistency}, 
\end{proof}

### Classification and Regression

\begin{algorithm}[H]
\label{alg:mlsm-fit}
\DontPrintSemicolon
\SetAlgoLined
\setstretch{1.1}
\KwData{Adjacency matrices $A^{(1)}, ..., A^{(L)}$, response variables $y_1, ..., y_L$, embedding dimension $d$, probability distribution family $F$, polynomial degree $R$, Isomap neighborhood parameters $\lambda_1, ..., \lambda_L$, stopping criterion $\epsilon$.}
\KwResult{Bezier coefficients $\hat{b}$, timepoint values $\hat{t}_1, ..., \hat{t}_n$.}
\For {$\ell = \{1, ..., L\}$} {
  Compute $\hat{X}^{(\ell)} \in \mathbb{R}^{n_\ell \times d}$, the $d$-dimensional ASE of $A^{(\ell)} \in \mathbb{R}^{n_\ell \times n_\ell}$.\;
  Initialize timepoints $\hat{t}_1, ..., \hat{t}_{n_\ell}$, first via a one-dimensional Isomap embedding of $\hat{X}^{(\ell)}$ using neighborhood parameter $\lambda_\ell$, then by normalizing the embedding to $[0, 1]$.\;
  \Repeat {the change in equation \ref{eq:bezier-ls} is less than $\epsilon$.} {
    Fit $\hat{b}$ by equation \ref{eq:bezier-ols}.\;
    \For {$i = 1, ..., {n_\ell}$} {
      Fit $\hat{t}_i$ by equation by finding the roots of equation \ref{eq:dbezier} and choosing the roots that minimizes equation \ref{eq:bezier-ls}.\;
    }
  }
  Estimate $\hat{\theta}_\ell$ from $\hat{t}_1, ..., \hat{t}_{n_\ell}$ via maximum likelihood estimation.\;
}
Fit a model for predicting $y_1, ..., y_L$ using the estimated parameters $\theta_1, ..., \theta_L$ (e.g., via linear regression).\;
\caption{Procedure for fitting a classification or regression model for an MLSM.}
\end{algorithm}

\begin{theorem}
\end{theorem}

# Simulation Study

## Classification

In this simulation experiment, the latent vectors were sampled along a Bezier curve defined by $g(t) = \bigl[ t^2 \; 2 t (1-t) \bigr]^\top$. 
The timepoints $t_i$ were sampled as iid Beta random variables with two sets of parameters, $(\alpha_1, \beta_1)$ and $(\alpha_2, \beta_2)$. 
The setup is as follows:

1. Draw response variables $y_1, ..., y_n \iid \Multinomial(1/2, 1/2)$.
2. For each $i = 1, ..., n$, draw $t_i \mid y_i \indep \Betadist(\alpha_{y_i}, \beta_{y_i})$, such that $\alpha_1 = 1$, $\beta_1 = 2$, $\alpha_2 = 2$, and $\beta_2 = 1$.
3. Construct each latent vector as $x_i = g(t_i)$ and compile them in data matrix $X = \bigl[ x_1 \; \cdots \; x_n \bigr]^\top$.
4. Sample graph and its adjacency matrix as $A \sim \RDPG(X)$. 

For each graph, we constructed the ASE, which was used to estimate the parameters $(\hat{\alpha}, \hat{\beta})$ for the graph, using the maximum likelihood method. 
The estimated parameters were then used as predictors $y_1, ..., y_n$, setting aside half for training and half for testing. 
We investigated graphs of size $|V| = 32, 64, 128, 256$. 
The number of graphs for each experiment was set to $L = 64$.
For each (number of graphs, size of graph) pair, we performed 32 replicates. 
Figure \ref{fig:class-sim-ase} shows the ASE of one graph.  
Figure \ref{fig:classification-sim} shows the boxplots of the classification error rates.

```{r class-sim-ase, cache = TRUE, fig.cap = 'One simulated graph (left) and its ASE (right). The red curve is the fitted quadratic Bezier curve on the ASE.'}
K <- 2
n <- 256

z <- 2

t. <- rbeta(n, 1, 1)
x1 <- t. ^ 2
x2 <- 2 * t. * (1 - t.)
X <- cbind(x1, x2)
P <- X %*% t(X)
A <- draw.graph(P)
Xhat <- embedding(A, 2, 0)

A.graph <- tidygraph::as_tbl_graph(unname(A), directed = FALSE)
graph.plot <- ggraph(A.graph, layout = 'auto') + 
  geom_edge_link(alpha = n ^ -.6) + 
  geom_node_point(show.legend = FALSE) + 
  labs(colour = NULL) + 
  theme_void() + 
  scale_colour_brewer(palette = 'Set1')

curve.est <- estimate.bezier.curve.2(Xhat, 
                                     degree = 2, 
                                     intercept = FALSE, 
                                     initialization = 'isomap', 
                                     normalize = TRUE)
ase.plot <- ggplot() + 
  geom_point(aes(x = Xhat[, 1], y = Xhat[, 2])) + 
  geom_line(aes(x = curve.est$X[, 1], y = curve.est$X[, 2]), 
             colour = 'red',
             size = .2) + 
  coord_fixed() + 
  labs(x = expression(x[1]), y = expression(x[2])) + 
  theme_bw()

gridExtra::grid.arrange(graph.plot, ase.plot, nrow = 1)
```

```{r classification-sim, cache = TRUE, fig.cap = 'Median classification error rate and its IQR vs. number of vertices in each graph.', fig.height = 2, fig.width = 4}
sim.df <- readr::read_csv('~/Documents/postdoc-first-project/classification-sim.csv')

n.vec <- unique(sim.df$n)

sim.df %>% 
  dplyr::group_by(n) %>% 
  dplyr::summarise(acc.med = median(acc),
                   acc.high = quantile(acc, .75),
                   acc.low = quantile(acc, .25)) %>% 
  ggplot() + 
  geom_line(aes(x = n, y = 1 - acc.med)) + 
  geom_point(aes(x = n, y = 1 - acc.med)) + 
  geom_errorbar(aes(x = n, ymin = 1 - acc.high, ymax = 1 - acc.low),
                width = .1) + 
  scale_x_log10(breaks = sort(n.vec)) +
  # scale_y_log10() +
  theme_bw() + 
  labs(x = 'number of vertices', y = 'error rate')
```

## Regression

In the first regression simulation, 

In the next regression simulation, we applied 

1. Draw angles $\theta_1, ..., \theta_N \iid \Uniform(\pi / 6, \pi / 3)$.
2. For each $k = 1, ..., N$, 
    i. Draw $t_1, ..., t_n \iid \Uniform(0, 1)$;
    ii. Draw $z_1, ..., z_n \iid \Multinomial(1/2, 1/2)$;
    iii. For each $i = 1, ..., n$, set $x_i = \begin{cases} \bigl[ t_i \; 0 \bigr]^\top \; z_i = 1 \\ \bigl[ t_i \cos \theta_\ell \; t_i \sin \theta_\ell \bigr]^\top \; z_i = 2 \end{cases}$;
    iv. Collect $X = \bigl[ x_1 \; \cdots \; x_n \bigr]^\top$ and draw $A \sim \RDPG(X)$;
    v. Set the response $y_\ell = \beta_0 + \beta_1 \theta_\ell$

In this simulation, we set $\beta_0 = \beta_1 = 1$. 
The number of graphs was set to $L = 64$, and the number of vertices per graph was set to $n = 128, 256, 512, 1024$. 
For each $n$, we simulated 32 replicates. 

```{r angle-reg-example, cache = TRUE}
n <- 512
a.vec <- c(1, 1)
b.vec <- c(1, 1)
theta.min <- pi / 6
theta.max <- pi / 3
beta1 <- 1
beta0 <- 1

z <- sample(seq(2), n, replace = TRUE)
t. <- rbeta(n, a.vec[z], b.vec[z])
theta <- runif(1, theta.min, theta.max)
y <- beta0 + beta1 * theta
x1 <- ifelse(z == 1, t., cos(theta) * t.)
x2 <- ifelse(z == 1, 0, sin(theta) * t.)
X <- cbind(x1, x2)
P <- X %*% t(X)
A <- draw.graph(P)
Xhat <- embedding(A, 2, 0)
clustering <- manifold.clustering(Xhat, 
                                  degree = 1,
                                  initialization = z,
                                  intercept = FALSE)
ase.plot <- plot.estimated.curves(Xhat, clustering) +
  scale_colour_brewer(palette = 'Set1')

A.graph <- tidygraph::as_tbl_graph(unname(A), directed = FALSE)
graph.plot <- ggraph(A.graph, layout = 'auto') + 
  geom_edge_link(alpha = n ^ -.6) + 
  geom_node_point(colour = 'black',
                  fill = 'white',
                  show.legend = FALSE) + 
  labs(colour = NULL) + 
  theme_void() + 
  scale_colour_brewer(palette = 'Set1')

gridExtra::grid.arrange(graph.plot, ase.plot, nrow = 1)
```

```{r angle-reg-results, cache = TRUE, fig.height = 2, fig.width = 4}
sim.df <- readr::read_csv('~/dev/multilayer-rdpg/simulations/angle-regression-sim.csv')
n.vec <- c(128, 256, 512, 1024)
N <- 64

sim.df %>% 
  dplyr::group_by(n) %>% 
  dplyr::summarise(mse.med = median(mse),
                   mse.high = quantile(mse, .75),
                   mse.low = quantile(mse, .25)) %>% 
  ggplot() + 
  geom_line(aes(x = n, y = mse.med)) + 
  geom_point(aes(x = n, y = mse.med)) + 
  geom_errorbar(aes(x = n, ymin = mse.high, ymax = mse.low),
                width = .1) + 
  scale_x_log10(breaks = sort(n.vec)) +
  # scale_y_log10() +
  theme_bw() + 
  labs(x = 'number of vertices', y = 'MSE')
```

# Applications

## Drosophila Connectome

In the second example, we analyzed the larval *Drosophila* mushroom body connectome \citep{Eichler141762}, which has been studied as a GRDPG by \citet{athreya2020estimation}. 
This dataset consists of two graphs representing two networks of neurons, one for each hemisphere of the *Drosophila* brain. 
In these graphs, each vertex is a neuron, and the labels correspond to one of four neuron types (Kenyon Cells, Input Neurons, Output Neurons, and Projection Neurons). 
The number of neurons in each hemisphere is not equal (209 in the left hemisphere and 213 in the right hemisphere).
The resulting graphs are illustrated in figure \ref{fig:mbconnectome-graph}. 

```{r mbconnectome-graph, fig.cap = 'Graphs of the Drosophila connectomes. The left and right are of the left and right hemispheres, respectively. Each vertex represents a neuron, which are labeled by neuron type.', fig.height = 2, figh.width = 12, cache = TRUE}
library(mbstructure)
data(MBconnectome)

graph.r <- generate.graph(newrdat, vdf.right)
A.r <- as.matrix(igraph::as_adjacency_matrix(graph.r$g, type = 'both'))
z.r <- as.numeric(graph.r$vdf$type)
n.r <- length(z.r)
A.r <- sign(A.r + t(A.r))
diag(A.r) <- 0
graph.r <- ggraph(A.r, layout = 'stress') + 
  geom_edge_link(alpha = 1 / sqrt(n.r) / 2) + 
  geom_node_point(aes(colour = factor(z.r)), 
                  show.legend = FALSE) + 
  labs(colour = NULL) + 
  theme_void() + 
  scale_colour_brewer(palette = 'Set1')

graph.l <- generate.graph(newldat, vdf.left)
A.l <- as.matrix(igraph::as_adjacency_matrix(graph.l$g, type = 'both'))
z.l <- as.numeric(graph.l$vdf$type)
n.l <- length(z.l)
A.l <- sign(A.l + t(A.l))
diag(A.l) <- 0
graph.l <- ggraph(A.l, layout = 'stress') + 
  geom_edge_link(alpha = 1 / sqrt(n.l) / 2) + 
  geom_node_point(aes(colour = factor(z.l)), 
                  show.legend = FALSE) + 
  labs(colour = NULL) + 
  theme_void() + 
  scale_colour_brewer(palette = 'Set1')

gridExtra::grid.arrange(graph.l, graph.r, nrow = 1)
```

```{r mbconnectome-ase, fig.cap = 'ASEs of the Drosophila connectome graphs. The embedding vectors are labeled by neuron type. ', fig.height = 2, fig.width = 8}
Xhat.l <- embedding(A.l, 2, 1)
ase.l <- ggplot() + 
  geom_point(aes(x = Xhat.l[, 1], y = Xhat.l[, 2],
                 colour = factor(z.l))) + 
  coord_fixed(.5) +
  theme_bw() + 
  theme(legend.position = 'none') +
  scale_colour_brewer(palette = 'Set1') + 
  labs(x = NULL, y = NULL, colour = NULL)

Xhat.r <- embedding(A.r, 2, 1)
ase.r <- ggplot() + 
  geom_point(aes(x = Xhat.r[, 1], y = Xhat.r[, 2],
                 colour = factor(z.r))) + 
  coord_fixed(.5) +
  theme_bw() + 
  theme(legend.position = 'none') +
  scale_colour_brewer(palette = 'Set1') + 
  labs(x = NULL, y = NULL, colour = NULL)

gridExtra::grid.arrange(ase.l, ase.r, nrow = 1)
```

When analyzed as a GRDPG, the ASE of each hemisphere suggests that nodes of each type falls along a curve in the latent space (figure \ref{fig:mbconnectome-ase}). 
In our analysis, we set the embedding dimension to $d = 4$, with 3 assortative dimensions and 1 disassortative dimension, and the figure only shows the first two assortative dimensions. 
This matches observations by \citet{athreya2020estimation}. 
We fit three latent structure Bezier curves, one for Kenyon Cells, one for Input and Output Neurons, and one for Projection Neurons, to the embedding for each hemisphere.
Then we fit a Beta distribution to to the timepoints along each curve and extracted the two Beta parameter estimates (via likelihood maximization) for each curve. 
If these Beta parameters are informative, we would expect the parameters for each hemisphere to match by neuron type, which is what we observe in these data (fig \ref{fig:mbconnectome-beta}). 
Using the three subgraphs of the left hemisphere as a training set to classify which subgraph of the right hemisphere belongs to which neuron type using the extracted Beta distribution parameter estimates as predictors results in 100\% accuracy. 

```{r mbconnectome-beta, fig.cap = 'Beta parameter estimates for each curve and hemisphere.', fig.height = 3, fig.width = 6}
param.est.df <- readr::read_csv('~/dev/multilayer-rdpg/drosophila.csv')

param.est.df %>% 
ggplot() + 
  geom_point(aes(x = shape1, y = shape2, colour = region, shape = hemisphere)) +
  labs(x = expression(hat(alpha)),
       y = expression(hat(beta)),
       colour = 'Neuron type',
       shape = 'Hemisphere') +
  coord_fixed() + 
  theme_bw()
```

## Human Connectome Project Aging Study

```{r load-hcp}
load('~/Downloads/Data/HCP/HCP-A/DTI/Data.RData')
z <- dti.info$hemisphere %>% 
  as.factor()
rois <- dti.info$aparc_aseg_region.names.short

ase.stats.df <- readr::read_csv('~/dev/multilayer-rdpg/hcp-a-eda.csv')
N <- nrow(ase.stats.df)
n <- nrow(dat.NumFibers[[1]])

ids <- ase.stats.df$id
```

In the second example, we analyzed fiber count data between brain regions from the Human Connectome Project (HCP). 
When analyzing these data as graphs, we denote the regions as vertices and the fiber counts between pairs of regions as weighted edges. 
A plausible statistical model for these data is to assume that the edge weights between pairs of vertices is Poisson distributed, i.e., the adjacency matrix is sampled as $A_{ij} \indep \Poisson(\Theta_{ij})$, where $\Theta \in \mathbb{R}_+^{n \times n}$ is a symmetric matrix of Poisson parameters. 

In this dataset, there are $L = `r N`$ graphs (corresponding to individual subjects), each with $n_\ell = n = `r n`$ vertices (corresponding to brain regions). 
Analyzing these graphs as RDPGs reveals that the DCBM is a good candidate for these data (figure \ref{fig:hcp-ase}). 

```{r hcp-ase, cache = TRUE, fig.cap = 'One graph from the HCP dataset (left) and its ASE (right). In the ASE, the lines are fitted via $K$-curves clustering using degree = 1. The outputted clusters correspond exactly to the left (red) and right (blue) hemispheres.', fig.width = 15, fig.height = 5, out.width = '100%'}
i <- 10
A <- dat.NumFibers[[i]] %>% 
  as.matrix()
A <- (A + t(A)) / 2
diag(A) <- 0

Xhat <- embedding(A, 2, 0, scale = TRUE)
clustering <- manifold.clustering(Xhat, 
                                  degree = 1,
                                  initialization = as.numeric(z),
                                  normalize = TRUE,
                                  intercept = FALSE,
                                  parallel = FALSE)

A.graph <- tidygraph::as_tbl_graph(unname(A), directed = FALSE)
graph.plot <- ggraph(A.graph, layout = 'auto') + 
  geom_edge_link(alpha = n ^ -.6) + 
  geom_node_point(aes(colour = factor(z)),
                  show.legend = FALSE) + 
  labs(colour = NULL) + 
  theme_void() + 
  scale_colour_brewer(palette = 'Set1')

ase.plot <- plot.estimated.curves(Xhat, clustering) +
  # ggrepel::geom_label_repel(aes(x = Xhat[, 1],
  #                               y = Xhat[, 2],
  #                               label = rois)) +
  scale_colour_brewer(palette = 'Set1')
gridExtra::grid.arrange(graph.plot, ase.plot, nrow = 1)
```

```{r age-lm}
set.seed(12345)

train.ind <- sample(c(TRUE, FALSE), nrow(ase.stats.df), replace = TRUE)
train.df <- ase.stats.df[train.ind, ]
test.df <- ase.stats.df[!train.ind, ]

angle.lm <- lm(age ~ angle.diff, data = train.df)
y.hat.angle <- predict(angle.lm, newdata = test.df)

degree.lm <- lm(age ~ deg.within + deg.between, data = train.df)
y.hat.deg <- predict(degree.lm, newdata = test.df)

dcbm.lm <- lm(age ~ B.ll + B.rr + B.lr, data = train.df)
y.hat.dcbm <- predict(dcbm.lm, newdata = test.df)

rmse.angle <- mean((test.df$age - y.hat.angle) ^ 2) ^ .5
rmse.deg <- mean((test.df$age - y.hat.deg) ^ 2) ^ .5
rmse.dcbm <- mean((test.df$age - y.hat.dcbm) ^ 2) ^ .5
```

```{r hcp-age-regression, fig.cap = 'Scatterplot between the subject age (in months) vs. the fitted angle between hemispheres of the brain in the latent space.'}
ggplot(ase.stats.df) + 
  geom_point(aes(x = angle.diff, 
                 y = age)) + 
  labs(x = 'angle between hemispheres in latent space (radians)',
       y = 'subject age (months)') + 
  theme_bw() + 
  stat_smooth(aes(x = angle.diff, y = age), method = 'lm')
```

```{r hcp-age-regression-degree, fig.cap = 'Scatterplot between the subject age (in months) vs. the average degree within (left) and between (right) hemispheres.', fig.width = 10, fig.height = 3, out.width = '100%'}
# w.plot <- ggplot(ase.stats.df) + 
#   geom_point(aes(x = deg.within, 
#                  y = age)) + 
#   labs(x = 'average degree within hemispheres',
#        y = 'subject age (months)') + 
#   theme_bw() + 
#   stat_smooth(aes(x = deg.within, y = age), method = 'lm')
# b.plot <- ggplot(ase.stats.df) + 
#   geom_point(aes(x = deg.between, 
#                  y = age)) + 
#   labs(x = 'average degree between hemispheres',
#        y = 'subject age (months)') + 
#   theme_bw() + 
#   stat_smooth(aes(x = deg.between, y = age), method = 'lm')
# gridExtra::grid.arrange(w.plot, b.plot, nrow = 1)
```

The ASE suggests a latent structure comprised of two Bezier curves of degree 1 (i.e., lines), one for each hemisphere of the brain, that meet at the origin. 
One possible parameter when analyzing these data as a MLSM is the angle between the two lines. 
The estimated angles were observed to correlate with the subject's age, with wider angles corresponding to older subjects (figure \ref{fig:hcp-age-regression}). 
A linear regression setting aside half of the brain connectivity graphs as test data achieves an RMSE of $`r round(rmse.angle, 3)`$ months. 

To compare this parameter as a covariate for age against other network statistics, we analyzed these data as a multilayer DCBM, first studied by \citep{agterberg2022joint}, who proposed the degree corrected multiple adjacency spectral embedding (DC-MASE) algorithm. 
Since these graphs come with hemisphere labels, we did not apply DC-MASE for community detection but instead used the estimators for the three edge connectivity parameters, $B_{LL}$, $B_{RR}$, and $B_{LR}$. 
A linear model trained on these parameter estimates achieves a higher RMSE of $`r round(rmse.dcbm, 3)`$ months, despite using three covariates instead of one. 
In addition, since the angles between the latent structures under the MLSM depend primarily on the shape of the latent structures rather than the exact community memberships, it does not depend on recovery of the original community labels. 
Ultimately, these two methods are estimators for transformations of the same parameters, and while we cannot determine how close these estimates are on the "true" edge connectivity parameters since they are unknown, we observe that the MLSM estimate is a better linear predictor than the rest. 

Other graph statistics were extracted for each brain network, and their correlations to age are reported in table \ref{tab:cor-comparison}. 
Note that aside from the angle between the latent structures under the MLSM model, these statistics assume that it is known which vertex belongs to which hemisphere. 

```{r}
pairwise.angles.df <- readr::read_csv('~/dev/multilayer-rdpg/pairwise-angles.csv')

dti.info.df <- dti.info %>% 
  dplyr::select(roi = aparc_aseg_region.names,
                rsn = RSN,
                hemisphere = hemisphere) %>% 
  dplyr::arrange(hemisphere, rsn)

pairwise.angles.df %<>%
  dplyr::mutate(roi1 = factor(roi1, levels = dti.info.df$roi),
                roi2 = factor(roi2, levels = dti.info.df$roi))
```

```{r cosine-heatmap, cache = TRUE, fig.width = 20, fig.height = 20, out.width = '100%', eval=FALSE}
# i <- ids[1]
# 
# pairwise.angles.df %>% 
#   dplyr::filter(id == i) %>% 
#   ggplot() +
#   geom_tile(aes(x = roi1, y = roi2, fill = angle)) +
#   viridis::scale_fill_viridis() +
#   coord_fixed() +
#   theme_minimal() +
#   labs(x = NULL, y = NULL) +
#   theme(axis.text.x = element_text(angle = 90, vjust = .5, hjust = 1))

# 36-46.5
# 46.5-56.5
# 56.5-69
# 69-90

pairwise.angles.df %>%
  dplyr::group_by(roi1, roi2) %>%
  dplyr::mutate(mean.angle = mean(angle)) %>%
  dplyr::ungroup() %>%
  dplyr::mutate(age = ifelse(age < 46.5, '36-46.5',
                             ifelse(age < 56.5, '46.5-56.5',
                                    ifelse(age < 69, '56.5-69',
                                           '69-90')))) %>%
  dplyr::group_by(age, roi1, roi2, rsn1, rsn2) %>%
  dplyr::summarise(angle = mean(angle) - mean.angle) %>%
  dplyr::ungroup() %>%
  ggplot() +
  geom_tile(aes(x = roi1, y = roi2, fill = angle)) +
  viridis::scale_fill_viridis() +
  coord_fixed() +
  theme_minimal() +
  theme(legend.position = 'bottom') +
  labs(x = NULL, y = NULL) +
  theme(axis.text.x = element_text(angle = 90, vjust = .5, hjust = 1),
        legend.key.width = unit(2, 'cm')) +
  facet_wrap(~ age)
```

```{r cosine-heatmap-2, cache = TRUE, fig.width = 10, fig.height = 10, out.width = '100%', eval=FALSE}
angles.plot.1 <- pairwise.angles.df %>%
  dplyr::filter(age < 46.5) %>%
  dplyr::group_by(roi1, roi2) %>%
  dplyr::summarise(angle = mean(angle)) %>%
  dplyr::ungroup() %>%
  ggplot() +
  geom_tile(aes(x = roi1, y = roi2, fill = angle)) +
  viridis::scale_fill_viridis() +
  coord_fixed() +
  theme_minimal() + 
  theme(legend.position = 'bottom') +
  theme(axis.text.x = element_text(angle = 90, vjust = .5, hjust = 1),
        legend.key.width = unit(2, 'cm')) +
  labs(x = NULL, y = NULL, title = '36-46.5') +
  theme(axis.text.x = element_text(angle = 90, vjust = .5, hjust = 1))
angles.plot.2 <- pairwise.angles.df %>%
  dplyr::filter(age < 56.5, age >= 36) %>%
  dplyr::group_by(roi1, roi2) %>%
  dplyr::summarise(angle = mean(angle)) %>%
  dplyr::ungroup() %>%
  ggplot() +
  geom_tile(aes(x = roi1, y = roi2, fill = angle)) +
  viridis::scale_fill_viridis() +
  coord_fixed() +
  theme_minimal() +
  theme(legend.position = 'bottom') +
  theme(axis.text.x = element_text(angle = 90, vjust = .5, hjust = 1),
        legend.key.width = unit(2, 'cm')) +
  labs(x = NULL, y = NULL, title = '46.5-56.5') +
  theme(axis.text.x = element_text(angle = 90, vjust = .5, hjust = 1))
angles.plot.3 <- pairwise.angles.df %>%
  dplyr::filter(age < 69, age >= 56.5) %>%
  dplyr::group_by(roi1, roi2) %>%
  dplyr::summarise(angle = mean(angle)) %>%
  dplyr::ungroup() %>%
  ggplot() +
  geom_tile(aes(x = roi1, y = roi2, fill = angle)) +
  viridis::scale_fill_viridis() +
  coord_fixed() +
  theme_minimal() +
  theme(legend.position = 'bottom') +
  theme(axis.text.x = element_text(angle = 90, vjust = .5, hjust = 1),
        legend.key.width = unit(2, 'cm')) +
  labs(x = NULL, y = NULL, title = '56.5-69') +
  theme(axis.text.x = element_text(angle = 90, vjust = .5, hjust = 1))
angles.plot.4 <- pairwise.angles.df %>%
  dplyr::filter(age >= 69) %>%
  dplyr::group_by(roi1, roi2) %>%
  dplyr::summarise(angle = mean(angle)) %>%
  dplyr::ungroup() %>%
  ggplot() +
  geom_tile(aes(x = roi1, y = roi2, fill = angle)) +
  viridis::scale_fill_viridis() +
  coord_fixed() +
  theme_minimal() +
  theme(legend.position = 'bottom') +
  theme(axis.text.x = element_text(angle = 90, vjust = .5, hjust = 1),
        legend.key.width = unit(2, 'cm')) +
  labs(x = NULL, y = NULL, title = '69-90') +
  theme(axis.text.x = element_text(angle = 90, vjust = .5, hjust = 1))

angles.plot.1
angles.plot.2
angles.plot.3
angles.plot.4

```

```{r roi-angles, cache = TRUE, out.width = '100%', fig.height = 10, fig.width = 10}
# pairwise.angles.df %>% 
#   dplyr::mutate(hemisphere1 = ifelse(grepl('(rh-)|(Right-)', roi1), 'R', 'L'),
#                 hemisphere2 = ifelse(grepl('(rh-)|(Right-)', roi2), 'R', 'L')) %>% 
#   dplyr::mutate(roi1 = gsub('(ctx-rh-)|(Right-)|(ctx-lh-)|(Left-)', '', roi1),
#                 roi2 = gsub('(ctx-rh-)|(Right-)|(ctx-lh-)|(Left-)', '', roi2)) %>% 
#   dplyr::filter(roi1 == roi2, hemisphere1 != hemisphere2) %>% 
#   ggplot() + 
#   geom_point(aes(x = age, y = angle, groups = roi1),
#              size = .1) +
#   facet_wrap(~ roi1) + 
#   theme_bw()
```

```{r cor-comparison, cache = TRUE}
assort.df <- foreach(id = ids, .combine = dplyr::bind_rows) %dopar% {
  A <- dat.NumFibers[[id]] %>% 
    as.matrix()
  A <- (A + t(A)) / 2
  diag(A) <- 0
  # A <- A / mean(A[upper.tri(A)])
  
  A.graph <- igraph::graph_from_adjacency_matrix(A) %>% 
    igraph::set_vertex_attr('hemisphere', value = z)
  
  assortativity <- igraph::assortativity(A.graph, z)
  n_triangles <- igraph::count_triangles(A.graph, z) %>% 
    unique()
  transitivity <- igraph::transitivity(A.graph)
  modularity <- igraph::modularity(A.graph, z)
  
  individual.stats.df <- dplyr::tibble(
    id = id,
    assortativity = assortativity,
    n_tri1 = n_triangles[1],
    n_tri2 = n_triangles[2],
    transitivity = transitivity,
    modularity = modularity
  )
  
  return(individual.stats.df)
} %>% 
  # dplyr::filter(!(id %in% ids.to.remove)) %>% 
  dplyr::inner_join(
    demo.data %>% 
      dplyr::filter(QC_Issue_Codes == '',
                    Full_MR_Compl == 1) %>% 
      dplyr::transmute(id = src_subject_id,
                       sex = sex,
                       age = as.numeric(interview_age)) %>% 
      dplyr::filter(age < 1200), 
    by = 'id'
  )

# https://github.com/jesusdaniel/JEG
source('~/dev/JEG/Code/joint_embedding.R')
A.list <- lapply(ids, function(id) {
  A <- dat.NumFibers[[id]] %>% 
    as.matrix()
  A <- (A + t(A)) / 2
  diag(A) <- 0
  return(A)
})
jeg.out <- multidembed(A.list, 2)
jeg.lm <- lm(ase.stats.df$age ~ jeg.out$lambda[, 1] + jeg.out$lambda[, 2])
jeg.yhat <- jeg.lm$fitted.values

dplyr::tibble(
  Metric = c('Angle between hemispheres',
             # 'Edge connectivity within left hemisphere',
             # 'Edge connectivity within right hemisphere',
             # 'Edge connectivity between hemispheres',
             'Degree within hemisphere',
             'Degree between hemispheres',
             # 'Number of triangles in left hemisphere',
             # 'Number of triangles in right hemisphere',
             # Assortativity w.r.t. hemisphere',
             # 'Transitivity',
             'Modularity w.r.t. hemisphere',
             'Joint Embedding'),
  Correlation = c(cor(ase.stats.df$age, ase.stats.df$angle.diff),
                  # cor(ase.stats.df$age, ase.stats.df$B.ll),
                  # cor(ase.stats.df$age, ase.stats.df$B.rr),
                  # cor(ase.stats.df$age, ase.stats.df$B.lr),
                  cor(ase.stats.df$age, ase.stats.df$deg.within),
                  cor(ase.stats.df$age, ase.stats.df$deg.between),
                  # cor(assort.df$age, assort.df$n_tri1),
                  # cor(assort.df$age, assort.df$n_tri2),
                  # cor(assort.df$age, assort.df$assortativity),
                  # cor(assort.df$age, assort.df$transitivity),
                  cor(assort.df$age, assort.df$modularity),
                  cor(assort.df$age, jeg.yhat)),
  `95% conf. int.` = c(cor.test(ase.stats.df$age, ase.stats.df$angle.diff)$conf.int %>% 
                         round(3) %>% 
                         paste(collapse = ', ') %>% 
                         paste0('(', ., ')'),
                       # cor.test(ase.stats.df$age, ase.stats.df$B.ll)$conf.int %>% 
                       #   round(3) %>% 
                       #   paste(collapse = ', ') %>% 
                       #   paste0('(', ., ')'),
                       # cor.test(ase.stats.df$age, ase.stats.df$B.rr)$conf.int %>% 
                       #   round(3) %>% 
                       #   paste(collapse = ', ') %>% 
                       #   paste0('(', ., ')'),
                       # cor.test(ase.stats.df$age, ase.stats.df$B.lr)$conf.int %>% 
                       #   round(3) %>% 
                       #   paste(collapse = ', ') %>% 
                       #   paste0('(', ., ')'),
                       cor.test(ase.stats.df$age, ase.stats.df$deg.within)$conf.int %>% 
                         round(3) %>% 
                         paste(collapse = ', ') %>% 
                         paste0('(', ., ')'),
                       cor.test(ase.stats.df$age, ase.stats.df$deg.between)$conf.int %>% 
                         round(3) %>% 
                         paste(collapse = ', ') %>% 
                         paste0('(', ., ')'),
                       # cor.test(assort.df$age, assort.df$n_tri1)$conf.int %>% 
                       #   round(3) %>% 
                       #   paste(collapse = ', ') %>% 
                       #   paste0('(', ., ')'),
                       # cor.test(assort.df$age, assort.df$n_tri2)$conf.int %>% 
                       #   round(3) %>% 
                       #   paste(collapse = ', ') %>% 
                       #   paste0('(', ., ')'),
                       # cor.test(assort.df$age, assort.df$assortativity)$conf.int %>% 
                       #   round(3) %>% 
                       #   paste(collapse = ', ') %>% 
                       #   paste0('(', ., ')'),
                       # cor.test(assort.df$age, assort.df$transitivity)$conf.int %>% 
                       #   round(3) %>% 
                       #   paste(collapse = ', ') %>% 
                       #   paste0('(', ., ')'),
                       cor.test(assort.df$age, assort.df$modularity)$conf.int %>% 
                         round(3) %>% 
                         paste(collapse = ', ') %>% 
                         paste0('(', ., ')'),
                       cor.test(assort.df$age, jeg.yhat)$conf.int %>% 
                         round(3) %>% 
                         paste(collapse = ', ') %>% 
                         paste0('(', ., ')'))) %>% 
  knitr::kable(digits = 3, caption = 'Correlation between age and various graph metrics.\\label{tab:cor-comparison}')
```

Next, we considered each hemisphere separately and fit a Beta distribution to the embedding of each half-network. 
The scatterplot of the fitted $\alpha$ and $\beta$ parameters suggests that there are two clusters of brain networks (figure \ref{fig:hcp-clustering}). 
Greater $\alpha$ values correspond to more embedding vectors farther away from the origin, corresponding to more nodes of higher degree, which in turn correspond to more brain regions with a greater number of connections. 
Conversely, greater $\beta$ values correspond to more embedding vectors closer to the origin, which in turn correspond to more brain regions with fewer connections. 
This may suggest that there are underlying groups, one of which tends to have higher connectivity in the left hemisphere, and the other having higher connectivity in the right hemisphere. 

```{r hcp-clustering, cache = TRUE, fig.cap = 'Scatterplot of fitted parameters of the Beta distribution. The left image are the parameters fitted on the embedding of the left hemisphere, and the right image are the parameters fitted on the embedding of the right hemisphere.', fig.width = 8, fig.height = 4}
beta.df <- ase.stats.df[c('a.l', 'a.r', 'b.l', 'b.r')]

zhat.l <- mclust::Mclust(beta.df[, c('a.l', 'b.l')], G = 2)$classification
plot.l <- ggplot(beta.df) + 
  geom_point(aes(x = a.l, y = b.l, colour = factor(zhat.l))) + 
  coord_fixed() + 
  theme_bw() + 
  labs(x = expression(hat(alpha)[l]), y = expression(hat(beta)[l]), colour = 'cluster') + 
  scale_colour_brewer(palette = 'Set1')

zhat.r <- mclust::Mclust(beta.df[, c('a.r', 'b.r')], G = 2)$classification
plot.r <- ggplot(beta.df) + 
  geom_point(aes(x = a.r, y = b.r, colour = factor(zhat.r))) + 
  coord_fixed() +
  theme_bw() + 
  labs(x = expression(hat(alpha)[r]), y = expression(hat(beta)[r]), colour = 'cluster') + 
  scale_colour_brewer(palette = 'Set1')

gridExtra::grid.arrange(plot.l, plot.r, nrow = 1)
```

# Discussion

\section*{Appendix A: Proofs of Theorems}

\newpage

# Bibliography